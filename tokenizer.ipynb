{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLtVNQh5oVhT1pUinRAsVr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qr0qXCkNcJD","executionInfo":{"status":"ok","timestamp":1710693123944,"user_tz":-60,"elapsed":266,"user":{"displayName":"David V","userId":"14796473427498895857"}},"outputId":"7690988d-a518-4705-c7bb-0524db2b4ffb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word index (word to token mapping):\n","{'they': 1, 'the': 2, 'and': 3, 'was': 4, 'a': 5, 'of': 6, 'had': 7, 'dursley': 8, 'were': 9, 'very': 10, 'in': 11, 'mr': 12, 'mrs': 13, 'to': 14, 'that': 15, 'much': 16, \"didn't\": 17, 'with': 18, 'called': 19, 'which': 20, 'he': 21, 'neck': 22, 'dursleys': 23, 'their': 24, 'it': 25, 'number': 26, 'four': 27, 'privet': 28, 'drive': 29, 'proud': 30, 'say': 31, 'perfectly': 32, 'normal': 33, 'thank': 34, 'you': 35, 'last': 36, 'people': 37, \"you'd\": 38, 'expect': 39, 'be': 40, 'involved': 41, 'anything': 42, 'strange': 43, 'or': 44, 'mysterious': 45, 'because': 46, 'just': 47, 'hold': 48, 'such': 49, 'nonsense': 50, 'director': 51, 'firm': 52, 'grunnings': 53, 'made': 54, 'drills': 55, 'big': 56, 'beefy': 57, 'man': 58, 'hardly': 59, 'any': 60, 'although': 61, 'did': 62, 'have': 63, 'large': 64, 'mustache': 65, 'thin': 66, 'blonde': 67, 'nearly': 68, 'twice': 69, 'usual': 70, 'amount': 71, 'came': 72, 'useful': 73, 'as': 74, 'she': 75, 'spent': 76, 'so': 77, 'her': 78, 'time': 79, 'craning': 80, 'over': 81, 'garden': 82, 'fences': 83, 'spying': 84, 'on': 85, 'neighbors': 86, 'small': 87, 'son': 88, 'dudley': 89, 'opinion': 90, 'there': 91, 'no': 92, 'finer': 93, 'boy': 94, 'anywhere': 95, 'everything': 96, 'wanted': 97, 'but': 98, 'also': 99, 'secret': 100, 'greatest': 101, 'fear': 102, 'somebody': 103, 'would': 104, 'discover': 105, 'think': 106, 'could': 107, 'bear': 108, 'if': 109, 'anyone': 110, 'found': 111, 'out': 112, 'about': 113, 'potters': 114}\n","\n","Tokenized sequence:\n","[12, 3, 13, 8, 6, 26, 27, 28, 29, 9, 30, 14, 31, 15, 1, 9, 32, 33, 34, 35, 10, 16, 1, 9, 2, 36, 37, 38, 39, 14, 40, 41, 11, 42, 43, 44, 45, 46, 1, 47, 17, 48, 18, 49, 50, 12, 8, 4, 2, 51, 6, 5, 52, 19, 53, 20, 54, 55, 21, 4, 5, 56, 57, 58, 18, 59, 60, 22, 61, 21, 62, 63, 5, 10, 64, 65, 13, 8, 4, 66, 3, 67, 3, 7, 68, 69, 2, 70, 71, 6, 22, 20, 72, 11, 10, 73, 74, 75, 76, 77, 16, 6, 78, 79, 80, 81, 82, 83, 84, 85, 2, 86, 2, 23, 7, 5, 87, 88, 19, 89, 3, 11, 24, 90, 91, 4, 92, 93, 94, 95, 2, 23, 7, 96, 1, 97, 98, 1, 99, 7, 5, 100, 3, 24, 101, 102, 4, 15, 103, 104, 105, 25, 1, 17, 106, 1, 107, 108, 25, 109, 110, 111, 112, 113, 2, 114]\n","\n","Vocabulary size (including +1 for potential 'unknown' token):\n","115\n"]}],"source":["from keras.preprocessing.text import Tokenizer\n","\n","# Sample text for tokenization\n","cleaned_text = \"\"\"\n","Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\n","They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\n","Mr. Dursley was the director of a firm called Grunnings, which made drills.\n","He was a big, beefy man with hardly any neck, although he did have a very large mustache.\n","Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors.\n","The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\n","The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it.\n","They didn't think they could bear it if anyone found out about the Potters.\n","\"\"\"\n","\n","# Tokenizing the text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([cleaned_text])\n","sequences = tokenizer.texts_to_sequences([cleaned_text])[0]\n","word_index = tokenizer.word_index\n","vocab_size = len(word_index) + 1\n","\n","# Output the results\n","print(\"Word index (word to token mapping):\")\n","print(word_index)\n","print(\"\\nTokenized sequence:\")\n","print(sequences)\n","print(\"\\nVocabulary size (including +1 for potential 'unknown' token):\")\n","print(vocab_size)\n"]}]}